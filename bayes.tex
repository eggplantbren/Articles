\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\E}{\mathds{E}}



\title{The role of statistics in propagating bad science}
\author{Brendon J. Brewer}

\begin{document}
\sffamily
\maketitle

If you regularly read the news, you will be familiar with the steady stream of
articles about scientific research. These articles usually reveal that some
popular food (such as chocolate)
is linked to a health outcome (either positive or negative).
The public reputation of science can suffer as
a result, as the state of knowledge seems to change more than it should
over time (``scientists can't make up their mind about whether butter is
healthy or not!''). This is regrettable,
but nobody would want to stop scientific results being reported. We all want
the best and most important science to filter through to the general public.

Yet it is unavoidable that there will always be
research which contains misleading or incorrect conclusions.
There are many ways this can happen. Research is difficult\footnote{As my
PhD supervisor used to say, {\em if it were easy they'd be doing it at UNSW},
a joke at the expense of a rival university.}, and sometimes
people simply make mistakes, or do experiments that aren't as well designed as
they could have been.
Scientists can be consciously or
unconsciously motivated to reach a certain desired conclusion.
Or a well-performed experiment might yield
misleading results simply by chance. This is magnified by the
{\em publication bias} effect, where the anomalous but interesting studies
are published and attract attention, while the boring yet accurate studies
remain in the file drawer.

The scientific community has developed processes which try to reduce these
problems. One of these is peer review, where one or more experts in the field
read and critique a paper before it is published.
This is an important part of the practice of
science, but being accepted by a journal
is not, and has never been, a guarantee that a paper is correct. Rather, it
is a simple filter used to try to prevent blunders.
In a broader sense, peer review is not just this pre-publication ritual, but
the idea that the scientific community will scrutinise and debate a result
even after it is published.

To further improve the practice of science, other practices are now being
advocated, such as pre-registration of clinical trials and
a greater focus on attempting to replicate the results of previous
experiments. These are promising ways of improving the signal-to-noise
ratio in the literature. Another is to improve the statistical
practices of scientists, so they can more reliably determine for themselves
how strong the
evidence is for their conclusions. If we can more competently determine
the degree to which a dataset provides evidence in favour of one theory over
another, we will end up with a clearer picture of what is known about the
world. The papers we write will be able to communicate more clearly the
appropriate amount of confidence or doubt associated with its conclusions.
The discipline of
statistics, and its newer sibling data science\footnote{My favourite
humourous definitions of data science are
{\em statistics done in San Francisco}, and {\em statistics done on a Mac}.
More seriously, Data Science consists of the overlap between statistics and
computer science.},
is broad and encompasses many different endeavours. For example, some researchers
work on developing practical software tools for analysing and visualising large and
complicated data sets, while others try to prove theorems about the mathematics
of probability. Many of us also collaborate with applied scientists and find
that even standard, well established methods that are straightforward in
principle can become research problems when you try to apply them to complex
situations. In this sense, statistics will always be a broad church.

Is it possible to calculate how strong the evidence is for one theory compared
to another? If so, what are the valid methods of doing so? This is the topic
of {\em inference}, which is an important part of the field of statistics.
Basically, we are concerned with trying to
draw conclusions from data without fooling ourselves. This is a noble goal
related to fundamental values of intellectual honesty and critical
thinking, and inference tries to formulate ways of doing this well.
Since many statisticians study and apply inference techniques,
we are perceived by the wider science community as authorities
in quantifying the strength of evidence. What outsiders may not realise is
that most statisticians frequently use, promote, and teach methods that are
obsolete, often illogical, unnecessarily confusing, and disconnected from the
needs of scientists. This has occurred because some fierce debates about the
nature of the concept of ``probability'' caused a rift in statistical thinking
in the 20th century. The methods of a single side of this debate became the
de-facto standard version of statistics and has been baked into undergraduate
teaching and the requirements of prestigious journals.

Probability is a set of mathematical rules relating some
numbers to other numbers. This much is uncontroversial, but things get more
tricky when we decide to apply probability equations to things in the real
world. Everyone acknowledges that the equations of probability can be applied
to {\em proportions} or {\em relative frequencies}. For example, the proportion
of people who ride a bicycle to work in Copenhagen is 30\% or 0.3.
The proportion of people who bicycle to work {\em and} are male is
$0.3 \times 0.5 = 0.15$ or 15$\%$. Here I used the {\em product rule} of
probabilities, and also an assumption of independence, which is empirical.
It may or may not hold in reality, and we could count people in Copenhagen if
we wanted to verify my calculation. To a frequentist, the mathematical notion
of probability can be applied to real world problems when we can find some
big set of things and ask questions about what fraction of that set has a
certain property.


%While
%it is impossible to be perfect at it, we should try to do the best
%we can.  If a new medical treatments yields 60
%recoveries out of 100 patients in a clinical trial, yet 45 out of 100 patients
%in the placebo group recover, how certain should we be that the drug really works?



%The problem is that most statisticians are wrong about how to do inference.

%Frequentist methods try to be right most of the time, whereas Bayesian methods
%try to honestly represent uncertainty every single time. Both of these seem
%like good ideas, and it isn't necessarily easy to see how they might be in
%conflict. After all, if we do something well every time, won't we tend to be
%right a lot of the time? And if we are right a lot of the time, many of the
%individual results will be correct, by definition.
%To see the distinction between these two ideas,
%consider the common critique of mainstream medicine which states that it
%doesn't treat patients as individuals.


%Mention being called a zealot and being on a religious crusade.
%Legit points: Bayesian moral community exists. But that doesn't mean we're
%wrong on facts. Pragmatists vs foundations.

%Heterodox academy still important.

\end{document}

