\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\E}{\mathds{E}}



\title{The responsibility of statistics in improving science}
\author{Brendon J. Brewer}

\begin{document}
\sffamily
\maketitle

Anyone who reads newspapers will be familiar with the steady stream of
articles about scientific research, usually revealing that some popular food
is ``linked'' to an unwanted medical condition,
or that a previously unhealthy substance, such as butter, is now
good for you. The public reputation of science and scientists can suffer as
a result, but nobody would want to stop scientific results being reported on.
Despite the peer review process being an important part of the practice of
science, the fact that a paper has been accepted by a journal
is not, and has never been, a guarantee that its conclusions are correct.
The real peer review comes later.

There are many reasons why a research paper might contain incorrect conclusions. Researchers may sometimes be consciously or
unconsciously motivated to reach a certain desired conclusion.
Or a well-performed experiment might yield
misleading results simply by chance. This is magnified by the
``publication bias'' effect, where the anomalous but interesting studies
are published or attract attention, while the boring yet accurate studies
remain in the proverbial file drawer. In recent years many scientists have
started paying greater attention to these problems, leading to welcome
developments such as pre-registration of clinical trials and
a greater focus on attempting to replicate the results of previous
experiments. These are promising ways of improving the signal-to-noise
ratio in the scientific literature.
However, there are also data analysis issues 

%A more appropriate response to the most recent study is
%to add its effect to your existing state of knowledge,
%which means you would shift your position slightly,
%rather than a completely reverse it every few months.


The disciplines of statistics and data science
(my favourite definition of which is {\em statistics done in San Francisco})
are broad and have many subfields. For example, some researchers
work on developing practical software tools for analysing and visualising large and
complicated data sets, while others try to prove theorems about the mathematics
of probability. Many of us also collaborate with applied scientists and find
that even applying standard, well established statistical methods to
real data can become a challenging research problem in itself. In this sense,
statistics and data science will always be a broad church.

However, one fundamental topic which has always been a large part of
statistics is {\em inference}, which is concerned with trying to
draw conclusions from data without fooling yourself. This is a noble goal
and is related to fundamental values of intellectual honesty and critical
thinking. While
it is impossible to be perfect at it, we should try to do the best
we can. Since many statisticians study and apply inference techniques,
we are perceived by the wider science community as authorities
in quantifying the strength of evidence. If a new medical treatments yields 60
recoveries out of 100 patients in a clinical trial, yet 45 out of 100 patients
in the placebo group recover, how certain should we be that the drug really works?



The problem is that most statisticians are wrong about how to do inference.

Frequentist methods try to be right most of the time, whereas Bayesian methods
try to honestly represent uncertainty every single time. Both of these seem
like good ideas, and it isn't necessarily easy to see how they might be in
conflict. After all, if we do something well every time, won't we tend to be
right a lot of the time? And if we are right a lot of the time, many of the
individual results will be correct, by definition.
To see the distinction between these two ideas,
consider the common critique of mainstream medicine which states that it
doesn't treat patients as individuals.


Mention being called a zealot and being on a religious crusade.
Legit points: Bayesian moral community exists. But that doesn't mean we're
wrong on facts. Pragmatists vs foundations.

Heterodox academy still important.



\end{document}

