\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\E}{\mathds{E}}



\title{The role of statistics in propagating bad science}
\author{Brendon J. Brewer}

\begin{document}
\sffamily
\maketitle

If you regularly read the news, you will be familiar with the steady stream of
articles about scientific research. These articles usually reveal that some
popular food (such as chocolate)
is linked to a health outcome (either positive or negative).
The public reputation of science can suffer as
a result, as the state of knowledge seems to change more than it should
over time (``scientists can't make up their mind about whether butter is
healthy or not!''). This is regrettable,
but nobody would want to stop scientific results being reported.
Rather, it would be better if the articles in the popular press mentioned
the uncertainties and complexities involved
and put the latest study in the context
of everything else that is known about the topic.

However, it is an unavoidable fact of life that there will always be
published research which contains misleading or incorrect conclusions.
There are many ways in which this can happen. Research is difficult and we
might simply make a mistake.
Researchers may sometimes be consciously or
unconsciously motivated to reach a certain desired conclusion.
Or a well-performed experiment might yield
misleading results simply by chance. This is magnified by the
{\em publication bias} effect, where the anomalous but interesting studies
are published or attract attention, while the boring yet accurate studies
remain in the file drawer.
The scientific community has developed processes which try to reduce these
problems. One of these is the peer review process, where an expert in the field reads and critiques
a paper before it is published.
Despite the peer review process being an important part of the practice of
science, the fact that a paper has been accepted by a journal
is not, and has never been, a guarantee that its conclusions are correct.
The real peer review comes later.

In recent years many scientists have
started paying greater attention to these problems, leading to welcome
developments such as pre-registration of clinical trials and
a greater focus on attempting to replicate the results of previous
experiments. These are promising ways of improving the signal-to-noise
ratio in the scientific literature. Another is to improve the statistical
practices of scientists, so they can more reliably determine how strong the
evidence is for their conclusions. If we can more competently 

The disciplines of statistics and data science
(my favourite definition of which is {\em statistics done in San Francisco})
are broad and have many subfields. For example, some researchers
work on developing practical software tools for analysing and visualising large and
complicated data sets, while others try to prove theorems about the mathematics
of probability. Many of us also collaborate with applied scientists and find
that even applying standard, well established statistical methods to
real data can become a challenging research problem in itself. In this sense,
statistics and data science will always be a broad church.

However, one fundamental topic which has always been a large part of
statistics is {\em inference}, which is concerned with trying to
draw conclusions from data without fooling yourself. This is a noble goal
and is related to fundamental values of intellectual honesty and critical
thinking. While
it is impossible to be perfect at it, we should try to do the best
we can. Since many statisticians study and apply inference techniques,
we are perceived by the wider science community as authorities
in quantifying the strength of evidence. If a new medical treatments yields 60
recoveries out of 100 patients in a clinical trial, yet 45 out of 100 patients
in the placebo group recover, how certain should we be that the drug really works?



The problem is that most statisticians are wrong about how to do inference.

Frequentist methods try to be right most of the time, whereas Bayesian methods
try to honestly represent uncertainty every single time. Both of these seem
like good ideas, and it isn't necessarily easy to see how they might be in
conflict. After all, if we do something well every time, won't we tend to be
right a lot of the time? And if we are right a lot of the time, many of the
individual results will be correct, by definition.
To see the distinction between these two ideas,
consider the common critique of mainstream medicine which states that it
doesn't treat patients as individuals.


Mention being called a zealot and being on a religious crusade.
Legit points: Bayesian moral community exists. But that doesn't mean we're
wrong on facts. Pragmatists vs foundations.

Heterodox academy still important.



\end{document}

