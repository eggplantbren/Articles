\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\E}{\mathds{E}}



\title{The role of statistics in propagating bad science}
\author{Brendon J. Brewer}

\begin{document}
\sffamily
\maketitle

If you regularly read the news, you will be familiar with the steady stream of
articles about scientific research. These articles usually reveal that some
popular food (such as chocolate)
is linked to a health outcome (either positive or negative).
The public reputation of science can suffer as
a result, as the state of knowledge seems to change more than it should
over time (``scientists can't make up their mind about whether butter is
healthy or not!''). This is regrettable,
but nobody would want to stop scientific results being reported. We all want
the best and most important science to filter through to the general public.

Yet it is unavoidable that there will always be
research which contains misleading or incorrect conclusions.
There are many ways this can happen. Research is difficult\footnote{As my
PhD supervisor used to say, {\em if it were easy they'd be doing it at UNSW},
a joke at the expense of a rival university.}, and sometimes
people simply make mistakes, or do experiments that aren't as well designed as
they could have been.
Scientists can be consciously or
unconsciously motivated to reach a certain desired conclusion.
Or a well-performed experiment might yield
misleading results simply by chance. This is magnified by the
{\em publication bias} effect, where the anomalous but interesting studies
are published and attract attention, while the boring yet accurate studies
remain in the file drawer.

The scientific community has developed processes which try to reduce these
problems. One of these is peer review, where one or more experts in the field
read and critique a paper before it is published.
This is an important part of the practice of
science, but being accepted by a journal
is not, and has never been, a guarantee that a paper is correct. Rather, it
is a simple filter used to try to prevent blunders.
In a broader sense, peer review is not just this pre-publication ritual, but
the idea that the scientific community will scrutinise and debate a result
even after it is published.

To further improve the practice of science, other practices are now being
advocated, such as pre-registration of clinical trials and
a greater focus on attempting to replicate the results of previous
experiments. These are promising ways of improving the signal-to-noise
ratio in the literature. Another is to improve the statistical
practices of scientists, so they can more reliably determine for themselves
how strong the
evidence is for their conclusions. If we can more competently determine
the degree to which a dataset provides evidence in favour of one theory over
another, we will end up with a clearer picture of what is known about the
world. The papers we write will be able to communicate more clearly the
appropriate amount of confidence or doubt associated with its conclusions.
The discipline of
statistics, and its newer sibling data science\footnote{My favourite
humourous definitions of data science are
{\em statistics done in San Francisco}, and {\em statistics done on a Mac}.
More seriously, Data Science consists of the overlap between statistics and
computer science.},
is broad and encompasses many different endeavours. For example, some researchers
work on developing practical software tools for analysing and visualising large and
complicated data sets, while others try to prove theorems about the mathematics
of probability. Many of us also collaborate with applied scientists and find
that even standard, well established methods that are straightforward in
principle can become research problems when you try to apply them to complex
situations. In this sense, statistics will always be a broad church.

Is it possible to calculate how strong the evidence is for one theory compared
to another? If so, what are the valid methods of doing so? This is the topic
of {\em inference}, which is an important part of the field of statistics.
Basically, we are concerned with trying to
draw conclusions from data without fooling ourselves. This is a noble goal
related to fundamental values of intellectual honesty and critical
thinking, and inference tries to formulate ways of doing this well.
Since many statisticians study and apply inference techniques,
we are perceived by the wider science community as authorities
in quantifying the strength of evidence. What outsiders may not realise is
that most statisticians frequently use, promote, and teach methods that are
obsolete, often illogical, unnecessarily confusing, and disconnected from the
needs of scientists. This has occurred because some fierce debates about the
nature of the concept of ``probability'' caused a rift in statistical thinking
in the 20th century. The methods of a single side of this debate became the
de-facto standard version of statistics and has been baked into undergraduate
teaching and the requirements of prestigious journals.

Probability is a set of mathematical rules relating some
numbers to other numbers. This much is uncontroversial, but things get more
tricky when we decide to apply probability equations to things in the real
world. Everyone acknowledges that the equations of probability can be applied
to {\em proportions} or {\em relative frequencies}. For example, the proportion
of people who ride a bicycle to work in Copenhagen is 30\% or 0.3.
The proportion of people who bicycle to work {\em and} are male is
$0.3 \times 0.5 = 0.15$ or 15$\%$. Here I used the {\em product rule} of
probabilities, and also an assumption of independence, which is empirical.
It may or may not hold in reality, and we could count people in Copenhagen if
we wanted to verify my calculation. To a frequentist, the mathematical notion
of probability can be applied to real world problems when we can find some
big set of things and ask questions about what fraction of that set has a
certain property.

To a Bayesian, the rules of probability can be applied to another real world
application, one with a much wider scope: reasoning in the presence of
uncertainty. In this situation, a probability is literally a numerical
description of how confident someone is that a proposition is true. For example,
I think the probability that Australia's Turnbull government will be
re-elected at the next election is about 80\%, or 0.8. This will either be true
or it won't, so my probability of 0.8 is not an observable property of
the universe, but is a property of my state of mind (although we could
discuss the observable properties of the universe that led me to assert 0.8
rather than 0.0001, 0.5, or 0.999). The `subjective' nature of Bayesian
probabilities was the main objection which led to the rise of frequentism in
the 20th century. I understand why people thought that science should stick
to more concrete, measurable ideas such as proportions, and not bother trying
to mathematise a vague notion like ``plausibility''.

Yet what happens if we relax our fears, and simply start using the Bayesian
framework? We end up with a mathematical theory which allows us to clearly
understand how ideal reasoning should behave when uncertainty is present. If we
are in a situation of uncertainty, and want to do the best we can, using the
rules of probability will ensure we meet bare minimum standards of rationality.
Many frequentist methods violate these standards.
For example, imagine a clinical trial where a new drug is being tested for
efficacy and is being compared to an old drug.
200 patients are recruited for the trial, 100
of which take the new drug, and 100 of which take the old drug. Since neither
drug is perfect, and drugs do not have exactly the same effect on everybody,
the exact number of people who in each group who recover is unpredictable.
Suppose 70 patients on the old drug are successfully treated, compared to
80 on the new drug. Based on this evidence, how sure can we be that the new
drug is better, and if so, by how much?

To make matters more explicit, we have to list all of the possible answers
we are considering to our question. Since we're talking about how effective
two drugs are, we can think of each drug having a number associated with it
which defines its effectiveness. Let's call the effectiveness of the first drug
$e_1$ and the effectiveness of the second drug $e_2$. Then our question boils
down to {\em what are the values of $e_1$ and $e_2$?} If we could answer that,
we could answer the broader question of whether $e_2$ is greater than $e_1$ and
if so, by how much. We usually think of these quantities by imagining what
fraction of patients would recover if we had a much larger version of the
experiment, with perhaps a million patients in each group.

In the Bayesian approach we are interested in a large set of hypotheses about
what the values of $e_1$ and $e_2$ might be. It's possible that $e_1=0.01$ and
$e_2 = 0.99$, i.e. that the new drug is amazing and the old one is terrible.
Another, more plausible possibility is that $e_1 = 0.65$ and $e_2=0.68$, i.e.
that both drugs are somewhat effective and drug two is slightly moreso. If we
consider $e_1$ and $e_2$ to two decimal places, then the total number of
possible values for each drug is 101 (if we include both 0.00 and 1.00 as
possibilities) is $101 \times 101 = 10201$.
Some of these possibilities are
quite plausible and others are not. We assign a number (a probability) to each
possibility, and the sum of all of these probabilities must be 1. A simple and
common default assumption is to assign a probability of $1/10201$ to each.
This is often a good enough assumption to get started, but if we want to put
more thought into it, we could choose something else, perhaps to reflect the
idea, discussed above, that it's plausible both drugs are moderately effective
and that there isn't a huge difference between them.
The technical name for the collection of
probabilities we choose to assign is the {\em prior distribution}.

With the data in hand (70/100 recoveries for the old drug and 80/100 for the
new), we can revise our probabilities. The usual terminology is that the
prior distribution will be replaced by the {\em posterior distribution}, which
is a collection of probabilities describing how plausible each hypothesis
(about the values of $e_1$ and $e_2$) is, taking into account the data.
The posterior probabilities are calculated by multiplying the prior probabilities
by so-called ``likelihoods'', one for each hypothesis,
which describe how likely our data would have been if the hypothesis were true.
Since the resulting probabilities will no longer add up to a total of 1, they
must be re-scaled so that they do.
Quite literally, those hypotheses that predicted the observed data well have
their probabilities increased, while those that did not become less
plausible.
There is an even simpler, but less widely known,
way of viewing the Bayesian updating procedure. It is equivalent to listing
all hypotheses you're considering, assigning a prior probability to each, and
deleting those which you've learned are false. The final step is to scale the
posterior probabilities to make sure the total is 1.

It is hard to imagine a simpler foundation for inference than that. Thankfully,
we don't need to: there are many arguments supporting the notion that this is
the only logically consistent way to make a mathematical model of uncertainty.
In practice, the results Bayesians get are literal answers to the question
``how confident should I be in this theory, as opposed to that one?''. All
disagreements are reduced to the question of what hypotheses we're talking
about, and what their prior probabilities are.

%Frequentist methods try to be right most of the time, whereas Bayesian methods
%try to honestly represent uncertainty every single time. Both of these seem
%like good ideas, and it isn't necessarily easy to see how they might be in
%conflict. After all, if we do something well every time, won't we tend to be
%right a lot of the time? And if we are right a lot of the time, many of the
%individual results will be correct, by definition.
%To see the distinction between these two ideas,
%consider the common critique of mainstream medicine which states that it
%doesn't treat patients as individuals.


%Mention being called a zealot and being on a religious crusade.
%Legit points: Bayesian moral community exists. But that doesn't mean we're
%wrong on facts. Pragmatists vs foundations.

% Sunk cost fallacy

%Heterodox academy still important.

\end{document}

