\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\E}{\mathds{E}}

\title{The responsibility of statistics in scientific reproducibility}
\author{Brendon J. Brewer}

\begin{document}
\maketitle

It is well known that public opinion can be pushed around by the latest study
finding that some popular food is ``linked'' to an unwanted medical condition,
or that a previously ``unhealthy'' substance, such as butter, is now
``good for you''. The scientific community is also learning that we can't lazily blame the media for this, and the fact that a paper has made it into a peer
reviewed journal is far from a guarantee that its conclusions are correct.
There are many reasons for this. Researchers may sometimes be consciously or
unconsciously motivated to reach a certain desired conclusion.
Sometimes, a well-performed experiment can yield
misleading results simply by chance. This is then magnified by the
``publication bias'' effect, where the anomalous studies are published
and/or noticed, while the more boring yet accurate studies remain in the
file drawer.

A more appropriate response to the most recent study is
to add its effect to your existing state of knowledge,
which means you would shift your position slightly,
rather than a completely reverse it every few months.


The disciplines of statistics and data science
(my favourite definition of which is {\it statistics done in San Francisco})
are broad and have many subfields. For example, some researchers
work on developing practical software tools for analysing and visualising large and
complicated data sets, while others try to prove theorems about the mathematics
of probability. Many of us also collaborate with applied scientists and find
that even applying standard, well established statistical methods to
real data can become a challenging research problem in itself. In this sense,
statistics and data science will always be a broad church.

However, one fundamental topic which has always been a large part of
statistics is {\it inference}, which is concerned with trying to
draw conclusions from data without fooling yourself. This is a noble goal
and is related to fundamental values of intellectual honesty and critical
thinking. While
it is impossible to be perfect at it, we should try to do the best
we can. Since many statisticians study and apply inference techniques,
we are perceived by the wider science community as authorities
in quantifying the strength of evidence. If a new medical treatments yields 60
recoveries out of 100 patients in a clinical trial, yet 45 out of 100 patients
in the placebo group recover, how certain should we be that the drug really works?



The problem is that most statisticians are wrong about how to do inference.

Frequentist methods try to be right most of the time, whereas Bayesian methods
try to honestly represent uncertainty every single time. Both of these seem
like good ideas, and it isn't necessarily easy to see how they might be in
conflict. After all, if we do something well every time, won't we tend to be
right a lot of the time? And if we are right a lot of the time, many of the
individual results will be correct, by definition.
To see the distinction between these two ideas,
consider the common critique of mainstream medicine which states that it
doesn't treat patients as individuals.


\end{document}

