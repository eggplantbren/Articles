\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{dsfont}
\usepackage{animate}
\usepackage{sectsty}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\allsectionsfont{\normalfont\sffamily}

\parindent=0cm

\title{Unscrambling the second law of thermodynamics}
\author{Brendon J. Brewer}

\begin{document}
\sffamily
\maketitle

The second law of thermodynamics surely qualifies as one of the most
talked-about principles in all of physics. Depending on who you ask, it is
either incredibly mysterious or fairly mundane. Some physicists think
the second law is connected to
fundamental ideas such as time and the origin of the universe
\citep{carroll}. Yet it is also an aspect of everyday experiences
such as the way a morning cup of coffee cools down,
or the fact that you cannot unscramble an egg.
The second law has even been invoked by rock band {\em Muse} to
explain why, in their
view, economic growth cannot continue for much longer \citep{muse}.
Yet trying to find a clear explanation of what the second law actually is
(and why it is true) can be a frustrating experience.

I first encountered the second law
as a teenager, while reading an issue of the fundamentalist Christian magazine
{\em Creation} given to me by my grandmother. Since the magazine wanted to
argue against biological evolution, it claimed that the second law of
thermodynamics does not allow evolution to occur. Its definition of the second
law was that {\em disorder always increases with time}.
At first glance, this does seem incompatible with evolution by
natural selection, which can lead to more complex,
``better designed'' organisms over time \citep{dawkins}.
I always thought it was unlikely that mainstream biology would flagrantly
contradict mainstream physics, so I remained sceptical of this argument,
even though I couldn't understand the explanations I found on the
internet at the time.

During my first undergraduate physics course, I was excited when we came to
study thermodynamics. Finally, I thought, I would be able to understand the
technical details of this fascinating and profound idea.
Alas, my expectations were not met, despite having a really good lecturer.
Instead of discussing big picture issues, we
worked out the maximum possible efficiency of refrigerators and steam engines
(which are probability interesting in their own ways, just not to me).
The version of the second law we studied was related to concepts of heat
and temperature, and little else.
The fact that heat always flows from a hot object to a cooler
one (and not the other way around) is a major everyday example. Your morning
cup of coffee cools down, and heats up the air around it: it doesn't heat
up further while cooling the room, even though that possibility is compatible
with other laws of physics such as the conservation of energy.

In thermodynamics, the second law is formalised by defining a quantity called
{\em entropy}. When heat flows {\em out of} an object and into another,
the first object's entropy goes
down, by an amount that depends on its temperature.
The change in the entropy is the amount of heat energy transferred,
$Q$ (usually measured in joules)
divided by the temperature of the object, $T_1$, measured in Kelvin.
When that same heat energy flows {\em into} another object, that object's
entropy goes up by $Q$ divided by the temperature of the
second object, $T_2$. The second law of thermodynamics can then be stated: if you
add up all of the changes in entropy of all the objects you are studying,
the result must be a positive number or zero. It can't be negative. In other
words, the total entropy must either increase or stay the same.
When a cup of coffee cools down its entropy decreases, but the entropy of its
surroundings increases by an even greater amount,
since the coffee is hotter than the surroundings.

The version of
the second law I just described, usually attributed to the 19th century
German physicist Rudolf Clausius, certainly has its uses. However it is a far
cry from the lofty fundamental principle I had expected to learn. What did it
have to do with evolution? The fact that organisms seem designed doesn't have
anything to do with heat being transferred. And it doesn't have much to do
with the economy either, except very indirectly because machines are useful
and the second law says some kinds of machines aren't possible.

Confusingly, as we learned the Clausius version of the second law, we also
discussed phenomena such as gas diffusion, shown in this animated gif:

%Animated gif in the public domain, obtained from:
%https://commons.wikimedia.org/wiki/File:Diffusion_animation.gif
\begin{figure}[ht!]
\centering
\animategraphics[scale=0.5]{12}{frame-}{0}{108}
\end{figure}

In the animation, purple gas atoms start above the horizontal barrier in the
middle of the box, and green atoms start below the barrier.
As time progresses, the purple and green atoms end up spread throughout the entire
box. This ``spreading out'', in a physical sense, was claimed to be an example
of the second law. Yet I could never find what heat was being transferred
in this example. If no heat is being transferred, the Clausius second law
doesn't apply --- we are left with hand waving about disorder.

So {\em is there} a version of the second law that relates to concepts more
general than heat and temperature?
It turns out the answer is yes, but I had to wait many years before I learned it.
Surprisingly, it turns out this more general second law isn't really a
principle of {\em physics}, but rather a
principle of {\em honest reasoning}. And this more general version of the
second law not only explains why the Clausius version is true, but gives us
a tool for much more general questions --- like the evolution question.
It also appears in everyday life, and not just in situations involving heat and
temperature. For example,
why is darts difficult? Why can't most men sing operatic high Cs?
And why are political polls (somewhat) accurate?

\section*{Uncertainty and volume}
During my PhD studies, I became intensely interested in Bayesian statistics
\citep{brewer}
and how to use it in astronomical data analysis. During this process I
discovered the work of the heterodox physicist E. T. Jaynes \citep{jaynes_site}
(1922--1988), whose clear-headed thinking and combative writing style
changed the direction of my PhD project and my life.

One day I came across a particular Jaynes paper, which is now one of my
favourite journal articles of all time
\citep{jaynes}. I didn't understand it
immediately, but had a strong sense that I should persist because it seemed
important. Every so often, I'd return to re-read it, understanding
just a little more each time. The breakthrough came after dinner one
night.

I had taken out a tub of ice-cream from the freezer for dessert. After dishing
the ice-cream into a bowl, I tried putting the tub back in the freezer, but
it wouldn't fit. Suddenly it clicked: Jaynes was explaining that
the second law of thermodynamics, both the Clausius heat/temperature version
and (importantly) a more general version, reduce to the
principle that {\em big things cannot fit into small spaces unless they are
compressed}. This is common sense when applied to physical objects, but to
get the second law, you have to apply it to an abstract object: a volume
of possibilities. This idea did not necessarily originate with Jaynes, as it
can be found (albeit less clearly stated)
in the work of chemist J. Willard Gibbs, among others.

To demonstrate the idea, consider the 52 books on my office bookshelf\footnote{This
is actually true, and it's a coincidence that there are 52 cards in a standard
deck of cards. I try to avoid explanations involving cards, coin flipping, and
dice, because people think they understand those already.}.
Only two of the books on the shelf are fiction, since I prefer to read
non-fiction, and it also makes more sense to have non-fiction books in an
office. Now I tell you that one of the books on the shelf was
signed by the author. Which one? I'm not telling. Based solely on this
information, it makes sense for you to assign a probability of 1/52 to each of the
books, describing how plausible you think it is for each book to be the signed
one.

From this state of near-ignorance, it seems like you might not be able to draw
any conclusions with a high confidence. But that's an illusion: it depends on
what questions you ask. Imagine if I were to ask you whether the signed book
is a non-fiction book. If your probability is 1/52 for each book being the
signed one, the probability the signed one is non-fiction must be
50/52 (adding the probabilities of the 50 non-fiction books together), 
or approximately 96\%. That's an impressive level of confidence --- much more than you should have in the conclusion of a single peer-reviewed science paper!
Of course, a high probability doesn't mean the result (the signed book is
non-fiction) is guaranteed. It just means it's very plausible {\em based on the
information that you explicitly put into the calculation}.

Here is a (very simple) diagram of my bookshelf, with the blue regions
being non-fiction books, and the red being fiction:
\begin{figure}[ht!]
\centering
\includegraphics{bookshelf.png}
\end{figure}
Clearly the safest bet is that the signed book is non-fiction, simply because
there are more of them --- they occupy a large majority of the volume of
possibilities (which, in this case, corresponds to a physical volume on my
bookshelf). A safe bet is, however, not a guarantee.

Now, what happens if I rearrange the books in an arbitrary manner? All the books
will be in a different order, and in terms of fiction vs. non-fiction, the
latter will still dominate:
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.61]{bookshelf2.png}
\end{figure}
After this, there's still a 96\% probability the signed book is non-fiction.
The general principle here, which Jaynes pointed out, is this:
{\em If you consider all possibilities that are consistent with
the information you have, and the vast majority of those possibilities
imply a certain outcome that is meaningful to you, then that outcome is very
plausible}.

\section*{Clausius from Jaynes}
Discuss the Clausius second law as a consequence of the Jaynes one

When I say that I have a hot cup of coffee and that the air around it is cooler,
it seems like quite a specific statement. But in a certain sense it is actually
a very vague statement, in that it leaves out a large number of details about
what's actually happening. I didn't tell you the colour of the mug, whether
my window was open, whether the coffee was instant or not (usually instant with
soy milk and two pills of sucralose if you're wondering...I can handle the hate mail). More conventionally, it also left out details about the position and
velocity (speed and direction of movement) of every molecule in the mug and
the surrounding air. Not entirely: the high temperature of the coffee means its
molecules are moving fairly rapidly.

Based on this very vague information about the cup of coffee, can we predict
what will happen in the future? Our common sense and experience say yes, very
loudly. Hot cups of coffee cool down. Duh!

But to a physicist the standard way of predicting the future is to use
the laws of motion, which predict how particles (such as molecules of coffee
and air) will move around. The catch is that we need to give the {\it initial
conditions}: what are all the positions and velocities that we're making our
prediction {\it from}?

Since we don't know the initial conditions (we only have the `vague' information
about the temperature) we can't actually apply the standard laws of motion to
see what will happen in the future. Any prediction we make will be a
guess, or more formally, a probability. We're in a situation analogous to the
bookshelf one.

\begin{figure}
\centering
\includegraphics[scale=0.6]{diagrams.png}
\end{figure}

Interestingly, this explanation doesn't really explain much about time,
since notions of time were assumed in the explanation itself.



\section*{Entropy is about what is known, not what is there}
%{\textsf \textbf Write about how the same physical system can have different entropies}


I must admit that when I hear `entropy' from a physicist, without a long
set of qualifications, I don't know what they're talking about. You can listen
to a whole talk full of statements like ``the entropy of the universe was
low in the past''. I don't know what that means,
and I don't think it's entirely my fault.
There seems to be little recognition of the fact
that the {\em exact same physical system}
can have more than one entropy, depending on how we want to think about it.
What {\em rules of thumb} are we looking for? Different ones may exist, and
we'll find them by using different entropies.

It's worth thinking about whether the second law really does forbid evolution
by natural selection. We don't need to get particularly technical with the
concept of entropy in order to have a go at that. All we need to ask is whether
it's plausible that a population of self-replicating organisms will tend to
`improve' their survival and reproduction abilities over time.
The answer is yes \citep{mackay}, provided the mutation rate is sufficiently
low. And if the organisms reproduce sexually, the population's average fitness
will increase even faster.

\section*{The second law in ordinary life}
As promised, the more general second law can be applied to everyday questions.
One of my favourite analogies is singing. About ten years ago, I decided to
learn how to sing, since it seemed fun, and not having to carry an instrument
around has its advantages. Back then I couldn't sing very well and in particular
I couldn't sing above E4, the E just above middle C. This was frustrating but
very common: the male singers you hear on the radio tend to sing songs with
notes well above that, apparently with ease. Why couldn't I?

It turns out that singing high notes requires 

\begin{thebibliography}{999} % if there are less than 10 entries, enter a one digit number
\bibitem[]{brewer} The Great Statistical Schism. Quillette, 2015.
http://quillette.com/2015/11/13/the-great-statistical-schism/

\bibitem[]{carroll}
Carroll, Sean. From eternity to here: the quest for the ultimate theory of time. Penguin, 2010.

\bibitem[]{dawkins}
Dawkins, Richard. The blind watchmaker: Why the evidence of evolution reveals a universe without design. WW Norton \& Company, 1986.

\bibitem[]{dawkins2}
Dawkins, Richard. The information challenge.
http://www.skeptics.com.au/resources/articles/the-information-challenge/

\bibitem[]{jaynes}
Jaynes, Edwin T.
Gibbs vs Boltzmann entropies. American Journal of Physics 33.5 (1965): 391-398.

\bibitem[]{jaynes_site}
http://bayes.wustl.edu/

\bibitem[]{mackay}
David MacKay, Information Theory, Inference, and Learning Algorithms.
Chapter 19.
http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/265.280.pdf

\bibitem[]{muse}
Muse. The 2nd law.
http://www.allmusic.com/album/the-2nd-law-mw0002406726
\end{thebibliography}

\end{document}

